{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P51THkFaOpYh"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y-ogUpXDDpBX"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "target = 'enc_group'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cWaxHHM1vDgm"
      },
      "outputs": [],
      "source": [
        "def down_sampling_preprocessing(df):\n",
        "  df['unique_id'] = df.reset_index().index\n",
        "  df = df.loc[:,df.isnull().sum()/len(df) <0.3]\n",
        "\n",
        "  df = df[(df[\"gname\"] != 'Unknown')]\n",
        "\n",
        "\n",
        "  # list1 = [\"Irish Republican Army (IRA)\"]\n",
        "  # df = df[~df.gname.isin(list1)]\n",
        "  df['eventid'] = df['eventid'].astype('int64')\n",
        "  categorical = ['extended','crit1','crit2','crit3','doubtterr','alternative','alternative_txt','multiple','enc_country','enc_province',\\\n",
        "                 'country_txt','enc_region','region_txt','vicinity','specificity','enc_attacktype','attacktype','enc_weapon_type',               'weapon_type','enc_weapon_subtype','weapon_subtype','success','suicide','target_entity','enc_target','target_type',\n",
        "                 'target_subtype','enc_nationality','nationality','claimed','property','cross_border','ideological_international',\\\n",
        "                 'province','entity','group','hostages/kidnapping','city','INT_MISC','INT_ANY','individual']\n",
        "  numerical = ['eventid','year','month','day','latitude','longitude','killed','nkillter','wounded','province']\n",
        "  text = ['approxdate','summary','related','location','motive']\n",
        "  numerical = ['eventid','year','month','day','latitude','longitude','killed','nkillter','wounded']\n",
        "  text = ['approxdate','summary','related','location','motive']\n",
        "  df.rename(columns = {'iyear':'year','imonth':'month','iday':'day','region':'enc_region', \\\n",
        "                       'attacktype1_txt':'attacktype','attacktype1':'enc_attacktype',\\\n",
        "                       'provstate':'enc_province','country':'enc_country',\\\n",
        "                       'targtype1_txt':'target_type','targtype1':'enc_target',\\\n",
        "                       'targsubtype1_txt':'target_subtype', \\\n",
        "                       'target1':'target_entity','weaptype1':'enc_weapon_type', \\\n",
        "                       'weaptype1_txt':'weapon_type','weapsubtype1_txt':'weapon_subtype','weapsubtype1':'enc_weapon_subtype',\\\n",
        "                       'corp1':'entity','natlty1':'enc_nationality','natlty1_txt':'nationality',\\\n",
        "                       'gname':'enc_group','nkill':'killed','nwound':'wounded',\\\n",
        "                       'targsubtype':'enc_target_subtype',\\\n",
        "                       'ishostkid':'hostages/kidnapping','INT_LOG':'cross_border',\\\n",
        "                       'INT_IDEO':'ideological_international'}, inplace = True)\n",
        "  for item in categorical:\n",
        "      if item not in df.columns:\n",
        "          pass\n",
        "      else:\n",
        "          df[item] = df[item].astype('category')\n",
        "  df[\"enc_city\"] = df[\"city\"].cat.codes\n",
        "  df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "  df[\"enc_city\"] = df[\"city\"].cat.codes\n",
        "  df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "  df[\"enc_province\"] = df[\"enc_province\"].cat.codes\n",
        "  df[\"enc_province\"] = df[\"enc_province\"].astype('category')\n",
        "  df[\"country_txt\"] = df[\"country_txt\"].cat.codes\n",
        "  df[\"region_txt\"] = df[\"region_txt\"].cat.codes\n",
        "  df[\"enc_attacktype\"] = df[\"attacktype\"].cat.codes\n",
        "  df[\"enc_target_type\"] = df[\"target_type\"].cat.codes\n",
        "  df[\"enc_target_subtype\"] = df[\"target_subtype\"].cat.codes\n",
        "  df[\"enc_target_entity\"] = df[\"target_entity\"].cat.codes\n",
        "  df[\"enc_nationality\"] = df[\"nationality\"].cat.codes\n",
        "  df[\"enc_weapon_type\"] = df[\"weapon_type\"].cat.codes\n",
        "  df['attack_date'] = pd.to_datetime(df[['year','month','day']], errors = 'coerce')\n",
        "  df = df.drop(columns = ['targsubtype1','INT_ANY','year','dbsource','month','day','crit1','crit2','crit3',\"city\",'country_txt','region_txt','attacktype','target_type','target_subtype','target_entity','nationality','weapon_type'])\n",
        "\n",
        "  if \"weapdetail\" in df.columns:\n",
        "    df = df.drop(columns = ['weapdetail'])\n",
        "\n",
        "  if \"summary\" in df.columns:\n",
        "    df = df.drop(columns = ['summary'])\n",
        "\n",
        "  if \"scite1\" in df.columns:\n",
        "    df = df.drop(columns = ['scite1'])\n",
        "\n",
        "\n",
        "  if \"weapon_subtype\" in df.columns:\n",
        "    df = df.drop(columns = ['weapon_subtype'])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmHg2NHzOixg"
      },
      "source": [
        "# Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTWGAMhnOcDo",
        "outputId": "536b4801-7641-465f-a7e1-69eebb426ad5"
      },
      "outputs": [],
      "source": [
        "# Uncomment below two lines if data is placed on Google Drive or need to connect drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nzhcRnHrRrd"
      },
      "source": [
        "# Down sampling preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Xp9W7urbN3"
      },
      "source": [
        "# Get Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wNlD3cHuqpvN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_stats(df1):\n",
        "    # Calculate the number of attacks per group\n",
        "    group_attacks = df1['enc_group'].value_counts().reset_index()\n",
        "    group_attacks = group_attacks.sort_values(by='count', ascending=False)\n",
        "    # group_attacks = group_attacks.sort_values(by='count', ascending=False)\n",
        "    group_attacks.columns = ['gname', 'group_count']\n",
        "    group_attacks = group_attacks.head(20)\n",
        "    # group_attacks = group_attacks[group_attacks['group_count'] > 150]\n",
        "\n",
        "\n",
        "    # Assuming group_attacks is your DataFrame\n",
        "\n",
        "    # Calculate the sum of the counts\n",
        "    total_count = group_attacks['group_count'].sum()\n",
        "\n",
        "    # Calculate the fraction per 1000 rows\n",
        "    frac_per_1000 = round(1000 / total_count)\n",
        "\n",
        "    # Calculate the exact number of rows each group should contribute\n",
        "    group_attacks['fraction_per_1000'] = (group_attacks['group_count'] * frac_per_1000).round().astype(int)\n",
        "    de = group_attacks\n",
        "    # Calculate the total number of rows after rounding\n",
        "    total_rows = group_attacks['fraction_per_1000'].sum()\n",
        "\n",
        "    # If the total number of rows is less than 1000, distribute the remaining rows among groups\n",
        "    if total_rows < 1000:\n",
        "        remaining_rows = 1000 - total_rows\n",
        "        group_attacks['fraction_per_1000'] += np.floor(remaining_rows * (group_attacks['group_count'] / total_count))\n",
        "\n",
        "    # If the total number of rows is greater than 1000, reduce the number of rows in some groups\n",
        "    elif total_rows > 1000:\n",
        "        excess_rows = total_rows - 1000\n",
        "        group_attacks['fraction_per_1000'] -= np.ceil(excess_rows * (group_attacks['group_count'] / total_count))\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    downsampled_data = pd.DataFrame()\n",
        "\n",
        "    # Iterate through each group in the top 20 groups\n",
        "    for index, row in group_attacks.head(20).iterrows():\n",
        "        # Extract data for the current group\n",
        "        group_data = df1[df1['enc_group'] == row['gname']]\n",
        "\n",
        "        # Calculate the number of samples to take for the current group\n",
        "        num_samples = int(row[\"fraction_per_1000\"])\n",
        "\n",
        "        # Sample rows without replacement based on the calculated proportion\n",
        "\n",
        "        sampled_data = group_data.sample(n=num_samples, replace=False, random_state=42)\n",
        "\n",
        "        # Append the downsampled data to the overall downsampled_data DataFrame\n",
        "        # downsampled_data = downsampled_data.append(sampled_data)\n",
        "        downsampled_data = pd.concat([downsampled_data, sampled_data], ignore_index=True)\n",
        "\n",
        "    return downsampled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hu_WYkU-3lD"
      },
      "source": [
        "#Encoding preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rxxwtHXp9nJ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def encode_preprocessing(df):\n",
        "    \"\"\"\n",
        "    Preprocess the input DataFrame by applying various data cleaning and transformation steps.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Input DataFrame to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Preprocessed DataFrame.\n",
        "    \"\"\"\n",
        "    # Filter out rows where enc_group is 'Unknown'\n",
        "    # df = df[(df[\"enc_group\"] != 'Unknown')]\n",
        "    # df['unique_id'] = df.reset_index().index\n",
        "    # Convert 'eventid' column to int64\n",
        "    df['eventid'] = df['eventid'].astype('int64')\n",
        "    categorical = ['extended','crit1','crit2','crit3','doubtterr','alternative','alternative_txt','multiple','enc_country','enc_province',\\\n",
        "                 'country_txt','enc_region','region_txt','vicinity','specificity','enc_attacktype','attacktype','enc_weapon_type',               'weapon_type','enc_weapon_subtype','weapon_subtype','success','suicide','target_entity','enc_target','target_type',\n",
        "                 'target_subtype','enc_nationality','nationality','claimed','property','cross_border','ideological_international',\\\n",
        "                 'province','entity','group','hostages/kidnapping','city','INT_MISC','INT_ANY','individual']\n",
        "    numerical = ['eventid','year','month','day','latitude','longitude','killed','nkillter','wounded','province']\n",
        "    text = ['approxdate','summary','related','location','motive']\n",
        "    numerical = ['eventid','year','month','day','latitude','longitude','killed','nkillter','wounded']\n",
        "    text = ['approxdate','summary','related','location','motive']\n",
        "    df.rename(columns = {'iyear':'year','imonth':'month','iday':'day','region':'enc_region', \\\n",
        "                         'attacktype1_txt':'attacktype','attacktype1':'enc_attacktype',\\\n",
        "                         'provstate':'enc_province','country':'enc_country',\\\n",
        "                         'targtype1_txt':'target_type','targtype1':'enc_target',\\\n",
        "                         'targsubtype1_txt':'target_subtype', \\\n",
        "                         'target1':'target_entity','weaptype1':'enc_weapon_type', \\\n",
        "                         'weaptype1_txt':'weapon_type','weapsubtype1_txt':'weapon_subtype','weapsubtype1':'enc_weapon_subtype',\\\n",
        "                         'corp1':'entity','natlty1':'enc_nationality','natlty1_txt':'nationality',\\\n",
        "                         'gname':'enc_group','nkill':'killed','nwound':'wounded',\\\n",
        "                         'targsubtype':'enc_target_subtype',\\\n",
        "                         'ishostkid':'hostages/kidnapping','INT_LOG':'cross_border',\\\n",
        "                         'INT_IDEO':'ideological_international'}, inplace = True)\n",
        "    for item in categorical:\n",
        "        if item not in df.columns:\n",
        "            pass\n",
        "        else:\n",
        "            df[item] = df[item].astype('category')\n",
        "    df[\"enc_city\"] = df[\"city\"].cat.codes\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "    df[\"enc_city\"] = df[\"city\"].cat.codes\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "    df[\"enc_province\"] = df[\"enc_province\"].cat.codes\n",
        "    df[\"enc_province\"] = df[\"enc_province\"].astype('category')\n",
        "    df[\"country_txt\"] = df[\"country_txt\"].cat.codes\n",
        "    df[\"region_txt\"] = df[\"region_txt\"].cat.codes\n",
        "    df[\"enc_attacktype\"] = df[\"attacktype\"].cat.codes\n",
        "    df[\"enc_target_type\"] = df[\"target_type\"].cat.codes\n",
        "    df[\"enc_target_subtype\"] = df[\"target_subtype\"].cat.codes\n",
        "    df[\"enc_target_entity\"] = df[\"target_entity\"].cat.codes\n",
        "    df[\"enc_nationality\"] = df[\"nationality\"].cat.codes\n",
        "    df[\"enc_weapon_type\"] = df[\"weapon_type\"].cat.codes\n",
        "    df['attack_date'] = pd.to_datetime(df[['year','month','day']], errors = 'coerce')\n",
        "    df = df.drop(columns = ['targsubtype1','INT_ANY','year','dbsource','month','day','crit1','crit2','crit3',\"city\",'country_txt','region_txt','attacktype','target_type','target_subtype','target_entity','nationality','weapon_type'])\n",
        "\n",
        "    if \"weapdetail\" in df.columns:\n",
        "      df = df.drop(columns = ['weapdetail'])\n",
        "\n",
        "    if \"summary\" in df.columns:\n",
        "      df = df.drop(columns = ['summary'])\n",
        "\n",
        "    if \"scite1\" in df.columns:\n",
        "      df = df.drop(columns = ['scite1'])\n",
        "\n",
        "\n",
        "    if \"weapon_subtype\" in df.columns:\n",
        "      df = df.drop(columns = ['weapon_subtype'])\n",
        "     # Drop columns specified in 'text' list\n",
        "    text = ['targsubtype1', 'latitude', 'longitude', 'region_txt', 'corp1', 'weapsubtype1',\n",
        "             'country_txt', 'dbsource', 'summary', 'crit1', 'crit2', 'crit3', 'targtype1_txt',\n",
        "             'targsubtype1_txt', 'attacktype1_txt', 'natlty1_txt', 'weaptype1_txt', 'provstate',\n",
        "             'weapsubtype1_txt', 'scite1', 'scite2','scite3','related','addnotes','hostkidoutcome_txt','propcomment','resolution']\n",
        "    for i in text:\n",
        "        if i in df.columns:\n",
        "            df = df.drop(columns=[i])\n",
        "\n",
        "    # Convert 'enc_target' column to category and encode it\n",
        "    df[\"enc_target\"] = df[\"enc_target\"].astype('category')\n",
        "    df[\"enc_target\"] = df[\"enc_target\"].cat.codes\n",
        "\n",
        "    # Convert 'enc_city' column to category and encode it\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].cat.codes\n",
        "\n",
        "    # Fill missing values with 0 for specified columns\n",
        "    replac_na_values = ['nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'nwoundte']\n",
        "    for i in replac_na_values:\n",
        "        if i in df.columns:\n",
        "            df[i] = df[i].fillna(0)\n",
        "\n",
        "    # Rename 'city' column to 'enc_city'\n",
        "    df.rename(columns={'city': 'enc_city'}, inplace=True)\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].astype('category')\n",
        "    df[\"enc_city\"] = df[\"enc_city\"].cat.codes\n",
        "\n",
        "\n",
        "    # Convert all columns to categorical and then one-hot encode them\n",
        "    for item in df:\n",
        "        df[item] = df[item].astype('category')\n",
        "\n",
        "    unique_id = df[[\"unique_id\"]]\n",
        "\n",
        "    df[\"enc_group\"] = df[\"enc_group\"].str.replace(' ', '_')\n",
        "    enc_group = df[[\"enc_group\"]]\n",
        "\n",
        "    df = df.drop(columns=['eventid','unique_id','enc_group'])\n",
        "    df = pd.get_dummies(df)\n",
        "\n",
        "    # df.insert(0,\"unique_id\",unique_id)\n",
        "    df.insert(1,\"enc_group\",enc_group)\n",
        "    df.reset_index(inplace=True)\n",
        "    # print(type(df))\n",
        "    # print(df.columns)\n",
        "\n",
        "    df.columns.values[0] = \"unique_id\"\n",
        "    # print(df.head(5))\n",
        "    # Drop 'enc_group' and 'eventid' columns\n",
        "    # df = df.drop(columns=['enc_group', 'eventid'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p66bIDPbsNhZ"
      },
      "source": [
        "# Split and Save data to CSV\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vaE9Qx-5cWWE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def split_and_save_original_data(df, downsampled_data, partition):\n",
        "    # Calculate the number of rows for each set\n",
        "    print(\"Number of samples after pre-processing/down sampling\",downsampled_data.shape[0])\n",
        "    print(\"Number of samples after pre-processing/down sampling\",downsampled_data.shape[1])\n",
        "    _ids = downsampled_data.unique_id.to_list()\n",
        "    _data = df[df['unique_id'].isin(_ids)]\n",
        "    encoded_downsampled_data = encode_preprocessing(_data)\n",
        "    encoded_directory = f\"../test/{partition}/\"\n",
        "    if not os.path.exists(encoded_directory):\n",
        "        os.makedirs(encoded_directory)\n",
        "    encoded_downsampled_data.to_csv(os.path.join(encoded_directory, \"encoded_\"+partition + \".csv\"),index=False, encoding=\"ISO-8859-1\")\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "    total_rows = downsampled_data.shape[0]\n",
        "    #down_directory = f\"/content/drive/MyDrive/GTD/original/{partition}/\"+\"down_sampled\"+\"/\"\n",
        "    down_directory = f\"../test/{partition}/\"+\"down_sampled\"+\"/\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(down_directory):\n",
        "        os.makedirs(down_directory)\n",
        "\n",
        "    # Save the train DataFrame to CSV\n",
        "    downsampled_data.to_csv(os.path.join(down_directory,\"new_downsampled_\"+ partition + \".csv\"),index=False, encoding=\"ISO-8859-1\")\n",
        "    train_size = int(total_rows * 0.7)\n",
        "    val_size = int(total_rows * 0.1)\n",
        "\n",
        "    # Splitting the data into train, validation, and test sets\n",
        "    train_data = downsampled_data.iloc[:train_size]\n",
        "    val_data = downsampled_data.iloc[train_size:train_size + val_size]\n",
        "    test_data = downsampled_data.iloc[train_size + val_size:]\n",
        "    print(\"Number of Features after preprocessing and before encoding\")\n",
        "    save_data(train_data, val_data,test_data,partition)\n",
        "    print(\" \")\n",
        "\n",
        "    # Splitting the data into train, validation, and test sets\n",
        "    encoded_train_data = encoded_downsampled_data.iloc[:train_size]\n",
        "    encoded_val_data = encoded_downsampled_data.iloc[train_size:train_size + val_size]\n",
        "    encoded_test_data = encoded_downsampled_data.iloc[train_size + val_size:]\n",
        "    print(\"Enccoded Features\",encoded_downsampled_data.shape)\n",
        "    print(\" \")\n",
        "    print(\"Number of one hot encoded Features after preprocessing\")\n",
        "    save_data(encoded_train_data, encoded_val_data,encoded_test_data,partition,\"encoded\")\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "def save_data(train_data, val_data ,test_data,partition,folder=\"down_sampled\"):\n",
        "\n",
        "\n",
        "    # train_ids = train_data.unique_id.to_list()\n",
        "    # val_ids = val_data.unique_id.to_list()\n",
        "    # test_ids = test_data.unique_id.to_list()\n",
        "\n",
        "    # train_data = df[df['unique_id'].isin(train_ids)]\n",
        "\n",
        "    # Define the train directory path\n",
        "    #train_directory = f\"/content/drive/MyDrive/GTD/original/{partition}/\"+folder+\"/train/\"\n",
        "    train_directory = f\"../test/{partition}/\"+folder+\"/train/\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(train_directory):\n",
        "        os.makedirs(train_directory)\n",
        "\n",
        "    # Save the train DataFrame to CSV\n",
        "    train_data.to_csv(os.path.join(train_directory, partition + \".csv\"),index=False, encoding=\"ISO-8859-1\")\n",
        "\n",
        "    # val_data = df[df['unique_id'].isin(val_ids)]\n",
        "    # Define the validation directory path\n",
        "    #val_directory = f\"/content/drive/MyDrive/GTD/original/{partition}/\"+folder+\"/val/\"\n",
        "    val_directory = f\"../test/{partition}/\"+folder+\"/val/\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(val_directory):\n",
        "        os.makedirs(val_directory)\n",
        "\n",
        "    # Save the validation DataFrame to CSV\n",
        "    val_data.to_csv(os.path.join(val_directory, partition + \".csv\"),index=False, encoding=\"ISO-8859-1\")\n",
        "\n",
        "    # Define the test directory path\n",
        "    #test_directory = f\"/content/drive/MyDrive/GTD/original/{partition}/\"+folder+\"/test/\"\n",
        "    test_directory = f\"../test/{partition}/\"+folder+\"/test/\"\n",
        "\n",
        "    # test_data = df[df['unique_id'].isin(test_ids)]\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(test_directory):\n",
        "        os.makedirs(test_directory)\n",
        "\n",
        "    # Save the test DataFrame to CSV\n",
        "    test_data.to_csv(os.path.join(test_directory, partition + \".csv\"),index=False, encoding=\"ISO-8859-1\")\n",
        "\n",
        "    # Print the shapes of the resulting sets\n",
        "    # print(\"Train set shape:\", train_data.shape)\n",
        "    # print(\"Validation set shape:\", val_data.shape)\n",
        "    # print(\"Test set shape:\", test_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Data Preparation, train size\",train_data.shape)\n",
        "    print(\"Data Preparation, val size\",val_data.shape)\n",
        "    print(\"Data Preparation, test size\",test_data.shape)\n",
        "\n",
        "\n",
        "# split_and_save_data(downsampled_data, \"your_partition_name\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmdYODSVrhtv"
      },
      "source": [
        "# Iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6aDpz9iqkAN",
        "outputId": "64152092-577d-4c3f-b7b7-ea0d1c8f2d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------- data_1970_80 -------------------\n",
            "Number of samples before down sampling 12576\n",
            "Number of Features after down sampling 135\n",
            "(8597, 35)\n",
            "(988, 35)\n",
            "Number of samples after pre-processing/down sampling 988\n",
            "Number of samples after pre-processing/down sampling 35\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (691, 35)\n",
            "Data Preparation, val size (98, 35)\n",
            "Data Preparation, test size (199, 35)\n",
            " \n",
            "Enccoded Features (988, 3044)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (691, 3044)\n",
            "Data Preparation, val size (98, 3044)\n",
            "Data Preparation, test size (199, 3044)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n",
            "------------------- data_1981_95 -------------------\n",
            "Number of samples before down sampling 48676\n",
            "Number of Features after down sampling 135\n",
            "(32925, 35)\n",
            "(991, 35)\n",
            "Number of samples after pre-processing/down sampling 991\n",
            "Number of samples after pre-processing/down sampling 35\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (693, 35)\n",
            "Data Preparation, val size (99, 35)\n",
            "Data Preparation, test size (199, 35)\n",
            " \n",
            "Enccoded Features (991, 2977)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (693, 2977)\n",
            "Data Preparation, val size (99, 2977)\n",
            "Data Preparation, test size (199, 2977)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n",
            "------------------- data_96_2010 -------------------\n",
            "Number of samples before down sampling 38450\n",
            "Number of Features after down sampling 135\n",
            "(16900, 42)\n",
            "(988, 42)\n",
            "Number of samples after pre-processing/down sampling 988\n",
            "Number of samples after pre-processing/down sampling 42\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (691, 42)\n",
            "Data Preparation, val size (98, 42)\n",
            "Data Preparation, test size (199, 42)\n",
            " \n",
            "Enccoded Features (988, 4924)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (691, 4924)\n",
            "Data Preparation, val size (98, 4924)\n",
            "Data Preparation, test size (199, 4924)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n",
            "------------------- data_2011_12 -------------------\n",
            "Number of samples before down sampling 13598\n",
            "Number of Features after down sampling 135\n",
            "(5597, 43)\n",
            "(992, 43)\n",
            "Number of samples after pre-processing/down sampling 992\n",
            "Number of samples after pre-processing/down sampling 43\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (694, 43)\n",
            "Data Preparation, val size (99, 43)\n",
            "Data Preparation, test size (199, 43)\n",
            " \n",
            "Enccoded Features (992, 3868)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (694, 3868)\n",
            "Data Preparation, val size (99, 3868)\n",
            "Data Preparation, test size (199, 3868)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n",
            "------------------- data_2013_14 -------------------\n",
            "Number of samples before down sampling 28939\n",
            "Number of Features after down sampling 135\n",
            "(12153, 42)\n",
            "(989, 42)\n",
            "Number of samples after pre-processing/down sampling 989\n",
            "Number of samples after pre-processing/down sampling 42\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (692, 42)\n",
            "Data Preparation, val size (98, 42)\n",
            "Data Preparation, test size (199, 42)\n",
            " \n",
            "Enccoded Features (989, 3556)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (692, 3556)\n",
            "Data Preparation, val size (98, 3556)\n",
            "Data Preparation, test size (199, 3556)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n",
            "------------------- data_2015_17 -------------------\n",
            "Number of samples before down sampling 39452\n",
            "Number of Features after down sampling 135\n",
            "(22737, 42)\n",
            "(989, 42)\n",
            "Number of samples after pre-processing/down sampling 989\n",
            "Number of samples after pre-processing/down sampling 42\n",
            " \n",
            "Number of Features after preprocessing and before encoding\n",
            "Data Preparation, train size (692, 42)\n",
            "Data Preparation, val size (98, 42)\n",
            "Data Preparation, test size (199, 42)\n",
            " \n",
            "Enccoded Features (989, 3827)\n",
            " \n",
            "Number of one hot encoded Features after preprocessing\n",
            "Data Preparation, train size (692, 3827)\n",
            "Data Preparation, val size (98, 3827)\n",
            "Data Preparation, test size (199, 3827)\n",
            " \n",
            "----------------------------------------------------\n",
            " \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# List of partitions\n",
        "partitions = [\"data_1970_80\",\"data_1981_95\", \"data_96_2010\", \"data_2011_12\", \"data_2013_14\",\"data_2015_17\"]\n",
        "\n",
        "for partition in partitions:\n",
        "    # Read the data\n",
        "    print(\"------------------- \"+partition+\" -------------------\")\n",
        "    #df = pd.read_csv(f\"/content/drive/MyDrive/GTD/original/{partition}/{partition}.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
        "    df = pd.read_csv(f\"../original/{partition}/{partition}.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
        "    print(\"Number of samples before down sampling\",df.shape[0])\n",
        "    print(\"Number of Features after down sampling\",df.shape[1])\n",
        "    df1 = down_sampling_preprocessing(df)\n",
        "    print(df1.shape)\n",
        "    downsampled_data = get_stats(df1)\n",
        "    print(downsampled_data.shape)\n",
        "\n",
        "    split_and_save_original_data(df, downsampled_data, partition)\n",
        "\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\" \")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
